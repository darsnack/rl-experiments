{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (DQN) on Raw Pixel Data\n",
    "\n",
    "**Author:** Kyle Daruwalla\n",
    "\n",
    "This is an attempt to recreate the PyTorch DQN tutorial in Julia using Flux. We will be working the the `CartPole-v0` environment where the learning agent tries to balance a pole on a cart by pushing the cart left or right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolve imports\n",
    "using Plots, Images, ProgressMeter, IJulia\n",
    "using Flux, CuArrays, Zygote\n",
    "using Gym\n",
    "using DataStructures: CircularBuffer\n",
    "using Random: shuffle\n",
    "using Distributions: sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "First, we define our DQN model using the standard layers found [here](https://fluxml.ai/Flux.jl/stable/models/layers/). The model takes the difference between current and previous screen patches, then outputs the Q-values for each action: $Q(s, \\text{left})$ and $Q(s, \\text{right})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.outdims(::BatchNorm, isize) = isize\n",
    "\n",
    "function createdqn(height, width)\n",
    "    ksize = (3, 3)\n",
    "    ssize = 2\n",
    "    lsizes = [1, 16, 32, 32] # channel sizes passing through conv layers\n",
    "    \n",
    "    # conv layers\n",
    "    convs = [\n",
    "        # 3x3 kernel applied to 3 channel input w/ 16 channel output\n",
    "        Conv(ksize, lsizes[1] => lsizes[2], stride = ssize),\n",
    "        BatchNorm(lsizes[2], relu),\n",
    "        Conv(ksize, lsizes[2] => lsizes[3], stride = ssize),\n",
    "        BatchNorm(lsizes[3], relu),\n",
    "        Conv(ksize, lsizes[3] => lsizes[4], stride = ssize),\n",
    "        BatchNorm(lsizes[4], relu)\n",
    "    ]\n",
    "    \n",
    "    ffwidth, ffheight = foldl((i, m) -> Flux.outdims(m, i), convs; init = (width, height))\n",
    "    ffsize = Int(ffwidth * ffheight * lsizes[4])\n",
    "    Chain(\n",
    "        convs...,\n",
    "        x -> reshape(x, :, size(x, 4)),\n",
    "        Dense(ffsize, 2)\n",
    "    ) |> gpu\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Processing\n",
    "\n",
    "Gym.jl returns an $c \\times h \\times w$ array. We need to transform this into a $w \\times h \\times c$ array which is how Flux interprets RGB arrays. We'll also crop the image, since most of the pixels are just white background. Lastly, we resize the image, because the original size of $400 \\times 600$ would consume too much memory for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"showenv.jl\")\n",
    "reqwidth = 40\n",
    "        \n",
    "# Instantiate the env\n",
    "env = make(\"CartPole-v0\", :rgb, true)\n",
    "reset!(env)\n",
    "screen, _ = showenv(env, reqwidth)\n",
    "width, height, nchannels, _ = size(screen)\n",
    "println(\"width = $(width), height = $(height), channels = $(nchannels)\")\n",
    "colorview(Gray, permutedims(cpu(screen[:, :, 1, 1]), [2, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory and Replay\n",
    "\n",
    "Typically, as an episode progresses, all the state transitions are stored in a trace. The NN implementing the policy is trained in batches over this trace. Here, we define a convenient way of storing each transition which is defined by\n",
    "1. **state**: the current state\n",
    "2. **action**: the action taken\n",
    "3. **reward**: the reward received\n",
    "4. **nextstate**: the state transitioned to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Transition\n",
    "    state :: CuArray{Float32}\n",
    "    action :: Int64\n",
    "    reward :: Float64\n",
    "    nextstate :: CuArray{Float32}\n",
    "    done :: Bool\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also defined the concept of a trace as a \"memory\". This is simply a circular buffer for storing transitions. You can `push!` onto a memory as well as randomly `sample` from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracelength = 10000\n",
    "memory = CircularBuffer{Transition}(tracelength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "To train the model, we define two DQNs. First, we have a policy net. This network is responsible for calculating $Q(s, a)$. We make decisions based off the policy net. For stability, we also have a target net. The target net is used to compute $V(s_{t + 1}) = \\max_a Q(s_{t + 1}, a)$. We only train the policy net every transition. The target net simply copies the weights off the policy net over at the end of each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function copyweights!(source, sink)\n",
    "    Flux.loadparams!(sink, Flux.params(source))\n",
    "end\n",
    "\n",
    "policynet = createdqn(height, width)\n",
    "targetnet = createdqn(height, width)\n",
    "copyweights!(policynet, targetnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is defined as follows\n",
    "$$L(B) = \\frac{1}{|B|} \\sum_{\\delta \\in B} \\ell(\\delta)$$\n",
    "\n",
    "$$\\ell(\\delta) = \\begin{cases}\n",
    "    \\frac{1}{2} \\delta^2 & |\\delta| \\leq 1 \\\\\n",
    "    |\\delta| - \\frac{1}{2} & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "$$\\delta = Q(s, a) - (r(s) + \\gamma V(s_{t + 1}))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const γ = 0.999\n",
    "\n",
    "# helper functions to get the Q and V values for transitions\n",
    "function Q(transition::Transition; policy)\n",
    "    q = policy(transition.state)[transition.action]\n",
    "    return q\n",
    "end\n",
    "function V(transition::Transition; target)\n",
    "    r = transition.reward\n",
    "    v = transition.done ? 0f0 : maximum(target(transition.nextstate))\n",
    "    r + γ * v\n",
    "end\n",
    "\n",
    "huber(δ) = sum(map(x -> abs(x) <= 1 ? 0.5 * x^2 : abs(x) - 0.5, δ)) / length(δ)\n",
    "l(q, v) = huber(q .- v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will optimize according to ADAM with default rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = RMSProp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Policy\n",
    "\n",
    "We use a standard Q-learning policy. With probability $\\epsilon$, we perform a random action, and with probability $1 - \\epsilon$, we perform the action dictated by the policy net. $\\epsilon$ starts at 0.9 and decays exponentially to 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const ϵ_start = 0.9\n",
    "const ϵ_end = 0.05\n",
    "const ϵ_decay = 200\n",
    "function selectaction(state, iter; policy) :: Int64\n",
    "    e = rand()\n",
    "    ϵ = ϵ_end + (ϵ_start - ϵ_end) * exp(-1. * iter / ϵ_decay)\n",
    "    if e > ϵ\n",
    "        Flux.onecold(policy(state), [1, 2])[1]\n",
    "    else\n",
    "        rand([1, 2])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Execution\n",
    "\n",
    "We now train our model by running through multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 128\n",
    "nepisodes = 500\n",
    "iter = 0\n",
    "progbar = Progress(nepisodes)\n",
    "epruntime = []\n",
    "loss = []\n",
    "currloss = 0\n",
    "target_update_rate = 20\n",
    "target_update_init = target_update_rate\n",
    "target_update_decay = 5\n",
    "update_idx = 1 + target_update_rate\n",
    "plot_update_rate = 100\n",
    "for i in 1:nepisodes\n",
    "    reset!(env)\n",
    "\n",
    "    last_screen, _ = showenv(env, reqwidth)\n",
    "    curr_screen, _ = showenv(env, reqwidth)\n",
    "    state = curr_screen .- last_screen\n",
    "    \n",
    "    T = 0\n",
    "    done = false\n",
    "    while !done\n",
    "        # take an action based on the policy net\n",
    "        action = selectaction(state, iter; policy = policynet)\n",
    "        _, reward, done, _ = step!(env, action)\n",
    "        \n",
    "        # push most recent transition onto memory trace\n",
    "        last_screen = curr_screen\n",
    "        curr_screen, _ = showenv(env, reqwidth)\n",
    "        nextstate = curr_screen - last_screen\n",
    "        transition = Transition(state, action, reward, nextstate, done)\n",
    "        push!(memory, transition)\n",
    "        state = nextstate\n",
    "        \n",
    "        # only train if the trace is at least batch size\n",
    "        if length(memory) >= batchsize\n",
    "            # sample a batch worth of transitions\n",
    "            batch = sample(memory, batchsize, replace = false)\n",
    "            qs = cat(Q.(batch; policy = policynet)..., dims=2)\n",
    "            vs = cat(V.(batch; target = targetnet)..., dims=2)\n",
    "\n",
    "            # update the model based on batch\n",
    "            weights = Flux.params(policynet)\n",
    "            Zygote.hook(x -> min(1, max(-1, x)), weights)\n",
    "            Flux.train!(l, weights, [(qs, vs)], optim)\n",
    "            currloss = cpu(l(qs, vs))\n",
    "            push!(loss, currloss)\n",
    "        end\n",
    "        \n",
    "        T += 1\n",
    "        iter += 1\n",
    "    end\n",
    "    \n",
    "    push!(epruntime, T)\n",
    "    \n",
    "    # update target net based on policy net\n",
    "    if target_update_rate < 0\n",
    "        copyweights!(policynet, targetnet)\n",
    "    elseif i == update_idx\n",
    "        copyweights!(policynet, targetnet)\n",
    "        target_update_rate -= target_update_decay\n",
    "        update_idx = i + target_update_rate\n",
    "    end\n",
    "    \n",
    "    if i < 100\n",
    "        display(plot(plot(loss, title=\"Loss\", xlabel=\"Iteration\", ylabel=\"Loss\", label=\"\"),\n",
    "                     plot(epruntime, title=\"Total Reward\", xlabel=\"Episode #\", ylabel=\"Reward\", label=\"\"),\n",
    "                layout=2, size=(1200, 400)))\n",
    "        next!(progbar)\n",
    "        IJulia.clear_output(true)\n",
    "        IJulia.flush_all()\n",
    "    elseif i % plot_update_rate == 0\n",
    "        IJulia.clear_output(true)\n",
    "        display(plot(plot(loss, title=\"Loss\", xlabel=\"Iteration\", ylabel=\"Loss\", label=\"\"),\n",
    "                     plot(epruntime, title=\"Total Reward\", xlabel=\"Episode #\", ylabel=\"Reward\", label=\"\"),\n",
    "                layout=2, size=(1200, 400)))\n",
    "        next!(progbar)\n",
    "        IJulia.flush_all()\n",
    "    else\n",
    "        next!(progbar)\n",
    "        IJulia.flush_all()\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Here, we plot the runtime of each episode. Theoretically, the runtime should increase as the agent learns the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgruntime = [(1 / min(n, 100)) * sum(epruntime[max(1, n - 100):n]) for n in eachindex(epruntime)]\n",
    "plot(epruntime, title=\"Episode Runtime\", xlabel=\"Episode #\", ylabel=\"Runtime (iterations)\", label=\"Runtime\")\n",
    "plot!(avgruntime, label=\"Avg. Runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_base = \"dqn-raw-tu$(target_update_init)-$(target_update_decay)-b$(batchsize)-t$(nepisodes)\"\n",
    "savefig(filename_base * \"-runtime.png\")\n",
    "savefig(plot(loss, title=\"Loss\", xlabel=\"Iteration\", ylabel=\"Loss\", label=\"\"), filename_base * \"-loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DelimitedFiles\n",
    "\n",
    "open(filename_base * \"-runtime.csv\", \"w\") do io\n",
    "    writedlm(io, ep_runtime, ',')\n",
    "end\n",
    "open(filename_base * \"-loss.csv\", \"w\") do io\n",
    "    writedlm(io, loss, ',')\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "46ebf984bbaf4fa58ed1f0e53d759ab9",
   "lastKernelId": "3e071426-12ed-4a7d-bbcb-36aad6a07c5f"
  },
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
